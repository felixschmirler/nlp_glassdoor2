perplexity(lda_cons, dtm_cons)
topicmodels::logLik(lda_cons)
terms(lda_pros, 30) %>% view()
pros_cons3a <- pros_cons2 %>%
unnest_tokens(word, text_clean) %>%
anti_join(stop_words) %>%
#distinct() %>%
filter(!word %in% undesirable_words)
low_occurence <- pros_cons3a %>%
group_by(pros_cons, word) %>%
summarise(
n = n()
) %>%
filter(n < 10) %>%
select(word)
pros_cons3a %<>% anti_join(low_occurence)
#LDA ####
lda_data <- pros_cons3a %>%
transmute(
document = paste(id, pros_cons, sep = "_"),
word = word
) %>%
count(document, word, sort = TRUE) %>%
ungroup()
lda_data_pros <- pros_cons3a %>%
filter(pros_cons == "pros") %>%
transmute(
document = paste(id, pros_cons, sep = "_"),
word = word
) %>%
count(document, word, sort = TRUE) %>%
ungroup()
lda_data_cons <- pros_cons3a %>%
filter(pros_cons == "cons") %>%
transmute(
document = paste(id, pros_cons, sep = "_"),
word = word
) %>%
count(document, word, sort = TRUE) %>%
ungroup()
#find best amount of topics
dtm <- cast_dtm(lda_data, document, word, n)
dtm_pros <- cast_dtm(lda_data_pros, document, word, n)
dtm_cons <- cast_dtm(lda_data_cons, document, word, n)
result <- FindTopicsNumber(
dtm,
topics = seq(from = 2, to = 30, by = 1),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
control = list(seed = 77),
mc.cores = NA,
verbose = TRUE
)
FindTopicsNumber_plot(result)
result_pros <- FindTopicsNumber(
dtm_pros,
topics = seq(from = 2, to = 30, by = 1),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
control = list(seed = 77),
mc.cores = NA,
verbose = TRUE
)
FindTopicsNumber_plot(result_pros)
result_cons <- FindTopicsNumber(
dtm_cons,
topics = seq(from = 2, to = 30, by = 1),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
control = list(seed = 77),
mc.cores = NA,
verbose = TRUE
)
FindTopicsNumber_plot(result_cons)
FindTopicsNumber_plot(result_pros)
FindTopicsNumber_plot(result)
lda <- LDA(dtm, k = 6, method = "GIBBS", control = list(seed = 1234)) #7 or 11 OR 6
terms(lda, 30) %>% view()
perplexity(lda, dtm)
topicmodels::logLik(lda)
lda_pros <- LDA(dtm_pros, k = 5, method = "GIBBS", control = list(seed = 1234)) #4 OR 5 or 5
terms(lda_pros, 30) %>% view()
perplexity(lda_pros, dtm_pros)
topicmodels::logLik(lda_pros)
pros_cons2 <- pros_cons %>%
mutate(
text_clean = str_squish(text) %>%
str_replace_all("`|´|’", "'") %>%
replace_contraction() %>%
str_to_lower() %>%
lemmatize_strings() %>%
str_replace_all("free book", "free_book") %>%
str_replace_all("work life balance|work / life balance|work - life balance", "work_life_balance") %>%
str_replace_all("upper managment|senior management|top management", "senior_management") %>%
str_remove_all("[0-9]")
)
undesirable_words <- c("prh", "penguin", "random", "house", "pearson", "bbc", "netflix", "education", "technology", "puplish")
pros_cons3a <- pros_cons2 %>%
unnest_tokens(word, text_clean) %>%
anti_join(stop_words) %>%
#distinct() %>%
filter(!word %in% undesirable_words)
low_occurence <- pros_cons3a %>%
group_by(pros_cons, word) %>%
summarise(
n = n()
) %>%
filter(n < 10) %>%
select(word)
pros_cons3a %<>% anti_join(low_occurence)
#LDA ####
lda_data <- pros_cons3a %>%
transmute(
document = paste(id, pros_cons, sep = "_"),
word = word
) %>%
count(document, word, sort = TRUE) %>%
ungroup()
lda_data_pros <- pros_cons3a %>%
filter(pros_cons == "pros") %>%
transmute(
document = paste(id, pros_cons, sep = "_"),
word = word
) %>%
count(document, word, sort = TRUE) %>%
ungroup()
lda_data_cons <- pros_cons3a %>%
filter(pros_cons == "cons") %>%
transmute(
document = paste(id, pros_cons, sep = "_"),
word = word
) %>%
count(document, word, sort = TRUE) %>%
ungroup()
#find best amount of topics
dtm <- cast_dtm(lda_data, document, word, n)
dtm_pros <- cast_dtm(lda_data_pros, document, word, n)
dtm_cons <- cast_dtm(lda_data_cons, document, word, n)
pros_cons2 <- pros_cons %>%
mutate(
text_clean = str_squish(text) %>%
str_replace_all("`|´|’", "'") %>%
replace_contraction() %>%
str_to_lower() %>%
lemmatize_strings() %>%
str_replace_all("free book", "free_book") %>%
str_replace_all("work life balance|work / life balance|work - life balance", "work_life_balance") %>%
str_replace_all("upper managment|senior management|top management", "senior_management") %>%
str_remove_all("[0-9]")
)
undesirable_words <- c("prh", "penguin", "random", "house", "pearson", "bbc", "netflix")
pros_cons3a <- pros_cons2 %>%
unnest_tokens(word, text_clean) %>%
anti_join(stop_words) %>%
#distinct() %>%
filter(!word %in% undesirable_words)
low_occurence <- pros_cons3a %>%
group_by(pros_cons, word) %>%
summarise(
n = n()
) %>%
filter(n < 10) %>%
select(word)
pros_cons3a %<>% anti_join(low_occurence)
#LDA ####
lda_data <- pros_cons3a %>%
transmute(
document = paste(id, pros_cons, sep = "_"),
word = word
) %>%
count(document, word, sort = TRUE) %>%
ungroup()
lda_data_pros <- pros_cons3a %>%
filter(pros_cons == "pros") %>%
transmute(
document = paste(id, pros_cons, sep = "_"),
word = word
) %>%
count(document, word, sort = TRUE) %>%
ungroup()
lda_data_cons <- pros_cons3a %>%
filter(pros_cons == "cons") %>%
transmute(
document = paste(id, pros_cons, sep = "_"),
word = word
) %>%
count(document, word, sort = TRUE) %>%
ungroup()
#find best amount of topics
dtm <- cast_dtm(lda_data, document, word, n)
dtm_pros <- cast_dtm(lda_data_pros, document, word, n)
dtm_cons <- cast_dtm(lda_data_cons, document, word, n)
lda_cons <- LDA(dtm_cons, k = 5, method = "GIBBS", control = list(seed = 1234)) #4 OR 3
terms(lda_cons, 30) %>% view()
perplexity(lda_cons, dtm_cons)
topicmodels::logLik(lda_cons)
pros_cons2 <- pros_cons %>%
mutate(
text_clean = str_squish(text) %>%
str_replace_all("`|´|’", "'") %>%
replace_contraction() %>%
str_to_lower() %>%
lemmatize_strings() %>%
str_replace_all("free book", "free_book") %>%
str_replace_all("work life balance|work / life balance|work - life balance", "work_life_balance") %>%
str_replace_all("upper managment|senior management|top management", "senior_management") %>%
str_remove_all("[0-9]")
)
undesirable_words <- c("prh", "penguin", "random", "house", "pearson", "bbc", "netflix", "education", "technology", "publish")
pros_cons3a <- pros_cons2 %>%
unnest_tokens(word, text_clean) %>%
anti_join(stop_words) %>%
#distinct() %>%
filter(!word %in% undesirable_words)
low_occurence <- pros_cons3a %>%
group_by(pros_cons, word) %>%
summarise(
n = n()
) %>%
filter(n < 10) %>%
select(word)
pros_cons3a %<>% anti_join(low_occurence)
#LDA ####
lda_data <- pros_cons3a %>%
transmute(
document = paste(id, pros_cons, sep = "_"),
word = word
) %>%
count(document, word, sort = TRUE) %>%
ungroup()
lda_data_pros <- pros_cons3a %>%
filter(pros_cons == "pros") %>%
transmute(
document = paste(id, pros_cons, sep = "_"),
word = word
) %>%
count(document, word, sort = TRUE) %>%
ungroup()
lda_data_cons <- pros_cons3a %>%
filter(pros_cons == "cons") %>%
transmute(
document = paste(id, pros_cons, sep = "_"),
word = word
) %>%
count(document, word, sort = TRUE) %>%
ungroup()
#find best amount of topics
dtm <- cast_dtm(lda_data, document, word, n)
dtm_pros <- cast_dtm(lda_data_pros, document, word, n)
dtm_cons <- cast_dtm(lda_data_cons, document, word, n)
result <- FindTopicsNumber(
dtm,
topics = seq(from = 2, to = 30, by = 1),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
control = list(seed = 77),
mc.cores = NA,
verbose = TRUE
)
FindTopicsNumber_plot(result)
result_pros <- FindTopicsNumber(
dtm_pros,
topics = seq(from = 2, to = 30, by = 1),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
control = list(seed = 77),
mc.cores = NA,
verbose = TRUE
)
FindTopicsNumber_plot(result_pros)
result_cons <- FindTopicsNumber(
dtm_cons,
topics = seq(from = 2, to = 30, by = 1),
metrics = c("Griffiths2004", "CaoJuan2009", "Arun2010", "Deveaud2014"),
method = "Gibbs",
control = list(seed = 77),
mc.cores = NA,
verbose = TRUE
)
FindTopicsNumber_plot(result_cons)
lda_pros <- LDA(dtm_pros, k = 4, method = "GIBBS", control = list(seed = 1234)) #4 OR 5 or 5
terms(lda_pros, 30) %>% view()
perplexity(lda_pros, dtm_pros)
topicmodels::logLik(lda_pros)
lda_cons <- LDA(dtm_cons, k = 4, method = "GIBBS", control = list(seed = 1234)) #4 OR 3
terms(lda_cons, 30) %>% view()
perplexity(lda_cons, dtm_cons)
topicmodels::logLik(lda_cons)
terms(lda_pros, 30) %>% view()
lda_cons <- LDA(dtm_cons, k = 4, method = "GIBBS", control = list(seed = 1234)) #4 OR 3
terms(lda_cons, 30) %>% view()
perplexity(lda_cons, dtm_cons)
topicmodels::logLik(lda_cons)
lda_beta <- tidy(lda, matrix = "beta")
lda_cons <- LDA(dtm_cons, k = 3, method = "GIBBS", control = list(seed = 1234)) #4 OR 3
terms(lda_cons, 30) %>% view()
perplexity(lda_cons, dtm_cons)
topicmodels::logLik(lda_cons)
FindTopicsNumber_plot(result)
FindTopicsNumber_plot(result_pros)
FindTopicsNumber_plot(result_cons)
lda <- LDA(dtm, k = 7, method = "GIBBS", control = list(seed = 1234)) #7 or 11 OR 6
terms(lda, 30) %>% view()
perplexity(lda, dtm)
topicmodels::logLik(lda)
topics(lda)
lda_beta <- tidy(lda, matrix = "beta")
lda_gamma <- tidy(lda, matrix = "gamma")
View(lda_beta)
View(lda_beta)
#Explore LDA Topics
betas_pros <- tidy(lda_pros, matrix = "beta")
betas_cons <- tidy(lda_cons, matrix = "beta")
View(betas_pros)
terms(lda_pros, 30) %>% view()
gammas_pros <- tidy(lda_pros, matrix = "gamma")
gammas_cons <- tidy(lda_cons, matrix = "gamma")
View(gammas_pros)
View(pros_cons)
#Explore LDA Topics
betas_pros <- tidy(lda_pros, matrix = "beta")
pros_top_terms <- betas_pros %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
pros_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
betas_contras <- tidy(lda_contras, matrix = "beta")
betas_cons <- tidy(lda_cons, matrix = "beta")
cons_top_terms <- betas_cons %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
cons_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
lda <- LDA(dtm, k = 7, method = "GIBBS", control = list(seed = 1234)) #7 or 11
terms(lda, 30) %>% view()
#Explore LDA Topics
betas_pros <- tidy(lda_pros, matrix = "beta")
pros_top_terms <- betas_pros %>%
group_by(topic) %>%
slice_max(beta, n = 10) %>%
ungroup() %>%
arrange(topic, -beta)
pros_top_terms %>%
mutate(term = reorder_within(term, beta, topic)) %>%
ggplot(aes(beta, term, fill = factor(topic))) +
geom_col(show.legend = FALSE) +
facet_wrap(~ topic, scales = "free") +
scale_y_reordered()
View(pros_cons3a)
View(betas_cons)
View(gammas_pros)
View(lda_data_pros)
View(lda_data)
View(lda_data)
View(lda_data)
View(pros_cons3a)
View(gammas_cons)
terms(lda_cons, 40) %>% view()
gammas_pros %>% mutate(gamma2 = log(gamma), gamma3 = gamma^2) %>% view()
gammas_pros %>%
group_by(document) %>%
slice_max(gamma) %>% view()
lda_beta %>%
group_by(topic) %>%
slice_max(order_by = beta, n = 10) %>% view()
#Evaluating lda performance on data
augment(lda_pros, dtm_pros) %>% view()
#Evaluating lda performance on data
augment(lda_pros, dtm_pros) %>% group_by(term, .topic) %>% summarise(n = n()) %>% view()
#Evaluating lda performance on data
augment(lda_pros, dtm_pros) %>%
#group_by(term, .topic) %>%
#summarise(n = n()) %>%
view()
View(pros_cons3a)
View(lda_data_pros)
#analysing glassdoor reviews
#load packages
library(tidyverse)
library(tidytext)
library(wordcloud)
library(corrplot)
library(lubridate)
library(viridis)
library(qdap)
library(textstem)
library(gridExtra)
library(ggwordcloud)
library(topicmodels)
library(tm)
library(quanteda)
library(lexicon)
library(ldatuning)
library(magrittr)
#load data
gd <- read.csv("reviews_clean.csv") %>%
mutate(
year = factor(year(date), ordered = TRUE),
id = paste0("id", row_number()),
date = ymd(date)
)
#explore frequency of reviews over time
gd %>%
ggplot(aes(date)) +
geom_histogram() +
facet_wrap(~company)
#explore ratings
gd %>%
ggplot(aes(rating)) +
geom_histogram() +
facet_wrap(~company)
gd %>%
select(year, rating) %>%
mutate(rating = factor(rating, ordered = TRUE)) %>%
group_by(year, rating) %>%
summarise(n = n()) %>%
pivot_wider(names_from = c(rating), values_from = n) %>%
replace_na(list(`1` = 0, `2` = 0, `3` = 0, `4` = 0, `5` = 0)) %>%
mutate(
`0.5` = (600 - `1` - `2`) - `3` / 2,
`5.5` = (800 - `5` - `4`) - `3` / 2,
) %>%
pivot_longer(cols = 2:8, names_to = "rating", values_to = "number") %>%
mutate(
rating = factor(rating, levels = c("5.5", "5", "4", "3", "2", "1", "0.5"),  ordered = TRUE)
) %>%
ggplot(aes(year, number, fill = rating)) +
geom_bar(position="stack", stat="identity") +
scale_fill_manual(labels = c("", "5", "4", "3", "2", "1", ""), values = c("#00000000", viridis(5), "#00000000")) +
ggtitle("Glasdoor Ratings over Time") +
theme(panel.grid.minor.y = element_blank(),
axis.text.y = element_text(hjust = 1),
plot.margin = margin(1, 0.5, 0, 0, "cm")
) +
labs(x = "", y = "")
#explore sub ratings
gd %>%
pivot_longer(matches("career|compensation|culture|balance|management")) %>%
ggplot(aes(value, fill = name)) +
geom_bar() +
#facet_wrap(~company+name, nrow = 3, )
facet_grid(company ~ name, scales = "free_y") +
theme(legend.position = "none")
#Explore relationship between subratings and main rating
cor_gd <- cor(drop_na(gd[c("rating", "work_life_balance", "culture_values", "career_opportunities", "compensation_benefits", "senior_management")]))
corrplot.mixed(cor_gd)
#tidy pros and cons for text analysis
pros_cons <- gd %>%
select(id, company, pros, cons) %>%
pivot_longer(c(pros, cons), names_to = "pros_cons", values_to = "text") %>%
mutate(
nwords = str_count(text,  "\\S+")
)
pros_cons %>%
ggplot(aes(nwords)) +
geom_histogram()
sum(pros_cons$nwords)
pros_cons2 <- pros_cons %>%
mutate(
text_clean = str_squish(text) %>%
str_replace_all("`|´|’", "'") %>%
replace_contraction() %>%
str_to_lower() %>%
lemmatize_strings() %>%
str_replace_all("free book", "free_book") %>%
str_replace_all("work life balance|work / life balance|work - life balance", "work_life_balance") %>%
str_replace_all("upper managment|senior management|top management", "senior_management") %>%
str_remove_all("[0-9]")
)
undesirable_words <- c("prh", "penguin", "random", "house", "pearson", "bbc", "netflix", "education", "technology", "publish")
pros_cons3a <- pros_cons2 %>%
unnest_tokens(word, text_clean) %>%
anti_join(stop_words) %>%
#distinct() %>%
filter(!word %in% undesirable_words)
low_occurence <- pros_cons3a %>%
group_by(pros_cons, word) %>%
summarise(
n = n()
) %>%
filter(n < 10) %>%
select(word)
pros_cons3a %<>% anti_join(low_occurence)
View(pros_cons3a)
View(pros_cons3a)
View(pros_cons2)
pros_cons3a %>%
mutate(
document = paste(id, "_", pros_cons3a)
) %>% view()
View(pros_cons3a)
pros_cons %>%
mutate(
document = paste(id, "_", pros_cons3a)
) %>% view()
pros_cons %>%
mutate(
document = paste(id, "_", pros_cons)
) %>% view()
View(gammas_pros)
